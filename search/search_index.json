{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Hey, thanks for visiting my portfolio. This website houses a curated collection of technical writing/editing projects that I have worked on over the course of my career. If you would like to contact me with job opportunities or just have general questions, feel free to use the Github and LinkedIn icons in the lower-right corner to find me.</p> <p>This website was built using Python, MkDocs, and various extensions and plug-ins, and is currently hosted on GitHub Pages. All content has been created using a combination of Markdown, HTML, and CSS.</p> <p>Tip</p> <pre><code>Click the toggle button to the left of the search bar above to swap between light and dark mode!\n</code></pre>"},{"location":"#portfolio-links","title":"Portfolio Links","text":"<ul> <li>Technical 1: Data Availability via Snowflake</li> <li>Technical 2: Authentication Bridge Service</li> <li>SOP: Installing and Configuring the Act! Web API Database Connection</li> <li>Editing: Mountain Technology Symposium Newsletter</li> <li>Proposal: Changing software and hardware for the Retail department</li> <li>Nelnet API Reference Documentation (a docs-as-code approach to API reference documentation. My team currently maintains and updates this site based on API/feature enhancements that affect the endpoints that our customers may use.)</li> <li>About Me</li> </ul>"},{"location":"AboutMe/","title":"About Me","text":"<p>Thanks for checking out my portfolio site!</p> <p></p> <p>My name is Seth Medina and I've been a big fan of computers and technology since the 1st grade when my mom brought home a shiny new desktop computer. While she was using it to peruse the (new-to-the-world) Internet for new houses, I was excited to try out all of the video games that were being planted in the boxes of my favorite breakfast cereals (anyone remember Chex Quest?). My love for computers and tech has only grown over the course of my life.</p> <p>I currently work as an IT Business Analyst / Technical Writer for a division of Nelnet, the consumer loan finance company that might service your student loans (and mine). In this role, I am responsible for maintaining, updating, and advocating for the large internal knowledge base of technical information that is used by our business stakeholders. This means I work with cross-functional teams of developers, business analysts, and product owners during our two-week sprints, and I track the work being done using Jira. I am responsible for capturing technical information from 3 different teams covering specific areas of the product and work to incorporate any major changes in system functionality into the various areas of documentation that my team maintains.</p> <p>These areas include:</p> <ul> <li>a Confluence knowledge base</li> <li>an API reference website (maintained using a docs-as-code approach with a static site generator)</li> <li>three separate user guides for the different user portals</li> <li>a large spreadsheet file containing the relationships between data elements and the various reporting views created in Snowflake</li> </ul> <p>My background is a unique blend of various writing positions and roles in the world of Information Technology. In my early roles in IT, I worked in very hands-on environments, installing and configuring software across client computing environments and helping fix things when they went wrong. These were primarily software that used SQL database engines, so I became very familiar with databases and their tooling. Prior to my endeavors in IT, I worked as a writer for various companies in the social media and search engine spaces. These roles helped shape my ability to write for different audiences and understand how to get in the shoes of the reader.</p> <p>In most places I've worked, documentation was an afterthought, meaning I've had the opportunity to help build team processes and best practices for efficiently completing the various tasks associated with documentation. This also means that I've continuously advocated for users and internal stakeholders who would benefit greatly from having quality documentation at their fingertips.</p> <p>Technical writing and Information Technology have become true passions of mine. This is largely because:</p> <ul> <li>I love learning about complex topics and ideas.</li> <li>I love helping others accomplish their goals and projects.</li> <li>I love the technology and tooling involved with modern technical writing workflows.</li> <li>I love problem solving.</li> <li>I love organizing information and designing documents.</li> <li>I love words and language.</li> </ul>"},{"location":"AboutMe/#personal-details","title":"Personal details","text":"<ul> <li>I currently work remotely in New Mexico with my fiance.</li> <li>We have a 3-year-old dog named Maisie who we love quite a bit.</li> <li>I love outdoor recreation and am very active.<ul> <li>I completed a 400-mile bikepacking loop around Scotland in September of 2023 with a group of friends.</li> <li>I completed a 50-mile backpacking trip along the Bechler River in Yellowstone National Park in 2021.</li> <li>I grew up snowboarding, but taught myself to ski while working for Telluride Ski Resort.</li> <li>I played roller and ice hockey throughout high school and college and my favorite NHL team is the Avalanche!</li> </ul> </li> </ul>"},{"location":"Act_ConnectLink_Install/","title":"SOP","text":""},{"location":"Act_ConnectLink_Install/#installing-and-configuring-the-act-web-api-database-connection","title":"Installing and Configuring the Act! Web API Database Connection","text":""},{"location":"Act_ConnectLink_Install/#overview","title":"Overview","text":"<p>When a client has requested to have the Act! Web API configured for use with Act! Marketing Automation (AMA), Act! Premium accessed via web (APFW), or 3rd-party connections, you will need to create a secure connection from their Act! database to the cloud API endpoint.</p> <p>You have two options for creating this connection for an Act! database:</p> <ul> <li>Installing the Act! Connect Link software on the client's server or machine hosting the Act! database</li> <li>Installing Act! Premium for Web on the client's server / host machine and then installing and configuring an SSL certificate on that same machine</li> </ul> <p>This document helps the Act! consultant understand how to achieve both of these configurations.</p> <p>Note</p> <pre><code>The API connections for the Microsoft Outlook and Word add-ins (v22.1+) use a local service and do not\nrequire the endpoint to be configured to work properly. However, users do need to have the Web API Access\npermission added to their permissions in the Tools &gt; Manage Users menu.\n</code></pre>"},{"location":"Act_ConnectLink_Install/#the-differences-between-act-connect-link-and-an-ssl-certificate","title":"The differences between Act! Connect Link and an SSL certificate","text":"<p>Things to consider when deciding which method to use:</p> <ul> <li>Act! Connect Link is easier to configure, as you simply install a piece of software and open any necessary firewall ports.</li> <li>Using an SSL certificate requires more work to configure, as you need to obtain and then install the certificate before you can set the API endpoint URL using the appropriate batch file.</li> <li>Both options can be used in conjunction with Act! Premium (Web) to set up web access to an Act! database.</li> <li>Act! Support officially does not recommend nor support Act! Premium for Web configurations using Act! Connect Link to create an API endpoint URL (only recommended for use with non-web Act! Premium).</li> <li>SSL certificate configurations tend to be more stable and provide smoother functionality across services that require the API endpoint connection (especially when dealing with databases that are large or have large data sets).</li> </ul>"},{"location":"Act_ConnectLink_Install/#installing-and-configuring-act-connect-link","title":"Installing and configuring Act! Connect Link","text":"<p>Installing Act! Connect Link on a client's machine will create a secure ground-to-cloud connection from that machine to the cloud server being hosted by the company providing this service.</p> <p>To install Connect Link:</p> <ol> <li> <p>Download the installer from the Act! website and run the .exe file, accepting all of the default options.</p> </li> <li> <p>Once installed, a unique URL is generated for use as the database's API endpoint.</p> </li> <li> <p>To view the unique URL that has been generated, open the Act! software.</p> </li> <li> <p>Click the Act! Connect button in the column of options seen in the lower-left corner of the program window.</p> </li> <li> <p>If the connection is working properly, you will see the URL displayed along the top of the program window.</p> </li> <li> <p>Copy this link to your clipboard using the button under the URL (optional).</p> </li> </ol> <p></p> <p>Warning</p> <pre><code>The API URL should be updated in the configuration file in the Act! program folders automatically,\nbut occasionally it does not get set properly. If this happens, you will need to use the GetSetCloudAPIURL.bat file\nfound in the JWT token error KB article to set the value manually.\n</code></pre>"},{"location":"Act_ConnectLink_Install/#using-connect-link-with-apfw","title":"Using Connect Link with APFW","text":"<ul> <li> <p>If the machine you have installed Connect Link on is using Act! Premium for Web, ensure the Website Administration settings have all been configured and tested successfully.</p> <ul> <li>You can find these settings in Tools &gt; Website Administration...</li> </ul> </li> <li> <p>To test a successful API connection to the Act! database, copy and paste the API URL into a web browser. If the connection is working properly, the Act! Web API Documentation webpage will display. </p> <ul> <li>If the client is using Act! Marketing Automation, you can also attempt to open the AMA module in Act!.</li> </ul> </li> <li> <p>To verify a successful connection for clients using AMA to see:</p> <ul> <li>Create a blank test campaign in AMA and attempt to select an Act! Group for the campaign.</li> <li>If the groups in the database appear for selection, you have a working API connection.</li> </ul> </li> </ul> <p></p>"},{"location":"Act_ConnectLink_Install/#troubleshooting-the-act-connect-link-connection","title":"Troubleshooting the Act! Connect Link connection","text":"<p>The Connect Link software operates over Port 80 (http), which is typically blocked by most organizations.</p> <p>You can find a list of other ports to try opening if the connection issues persist by opening the Connect Link menu.</p> <ol> <li> <p>To open the Connect Link menu, click the Windows Start button and search for \"Act!\".</p> <p></p> </li> <li> <p>Click the Act! Connect Link option to launch the interface.</p> </li> <li> <p>Click the Config tab to view the different Connect Link settings / ports.</p> </li> </ol> <p></p>"},{"location":"Act_ConnectLink_Install/#uninstalling-reinstalling-connect-link","title":"Uninstalling / reinstalling Connect Link","text":"<p>If opening these ports for the Connect Link software does not help resolve your connection problems, the next step is to try uninstalling and reinstalling the software.</p> <ol> <li> <p>Open the installed programs list for the machine in question and uninstall the Connect Link software.</p> </li> <li> <p>Once the uninstall process finishes, navigate to the C: drive folder (or whatever the root drive may be) and delete the ActConnectLink folder.</p> </li> <li> <p>Reinstall the Connect Link software by running the installer file.</p> </li> </ol> <p>This will create a new endpoint / tenant for the Act! database and can help resolve some connectivity issues.</p> <p>Because this changes the API URL value, you may need to update the API URL for the database using the GetSetCloudAPIURL.bat file in the JWT token error KB article.</p> <p>Warning</p> <pre><code>Uninstalling &amp; reinstalling Connect Link will create a new Connect Link API web URL. If a client is using\nConnect Link for connections other than AMA (Act! Companion Mobile App, Zapier.com, etc.), they will need to be\nnotified to update their login credentials URL in those other programs.\n</code></pre>"},{"location":"Act_ConnectLink_Install/#configuring-the-api-url-endpoint-via-ssl-certificate","title":"Configuring the API URL endpoint via SSL certificate","text":"<p>If the client wants a customized web URL when accessing their Act! database via web or wants the stability benefits of using the SSL certificate configuration, then an SSL certificate will need to be purchased and installed on the machine hosting the Act! database.</p> <p>Typically AspenTech Act! support agents do not handle obtaining or installing the SSL certificate directly.</p> <p>Have the client's IT vendor / contact handle the installation of the SSL certificate and once it has been installed, the AspenTech consultant will be able to configure the API endpoint for the Act! database.</p> <p>Note</p> <pre><code>An SSL certificate configuration requires Act! Premium for Web be installed on the host machine, along with\nthe completion of all Website Administration settings in Tools &gt; Website Administration.\n</code></pre> <p>Before the SSL certificate is purchased, discuss with the client their desired domain name URL for their Act! website / API endpoint.</p> <p>Typically this is something like marketing.companymame.com or act.companyname.com, but can be whatever the client desires to an extent.</p> <p>This information will need to be provided to the IT vendor / contact so they can purchase an SSL certificate with the proper domain name.</p>"},{"location":"Act_ConnectLink_Install/#configuring-iis-once-the-ssl-certificate-has-been-installed","title":"Configuring IIS once the SSL certificate has been installed","text":"<p>Once IT has installed the SSL certificate on the host machine, you are ready to configure the API endpoint URL.</p> <p>To start, make sure the certificate is bound to the https port (443) in the IIS menu.</p> <ol> <li> <p>To open the IIS menu, click the Windows Start button and type \"IIS\".</p> </li> <li> <p>Click on Internet Information Services (IIS) Manager to launch the interface.</p> <p>Note</p> <pre><code>Do not click on Internet Information Services (IIS) 6.0 Manager.\n</code></pre> </li> <li> <p>In the Connections pane located in the left-hand column, expand each line until Default Website becomes visible.</p> </li> <li> <p>Click on Default Website to select it.</p> </li> <li> <p>In the Actions pane in the right-hand column, click Bindings under Edit Site.</p> </li> <li> <p>If there is no https binding added, click the Add... button to create a new site binding.</p> </li> <li> <p>Click the Type dropdown menu and choose https.</p> </li> <li> <p>Click the SSL certificate dropdown menu to select the appropriate SSL certificate that was installed by IT.</p> </li> </ol> <p>You should not need to change any other values for this binding if the SSL certificate was installed properly.</p> <p>If you do not see the SSL certificate in the dropdown menu for selection, it is likely the SSL certificate was not installed properly.</p> <p></p> <p>After the SSL certificate has been added to the site binding, we need to set the API URL value in the database web configuration file.</p>"},{"location":"Act_ConnectLink_Install/#setting-the-api-url-value-in-the-web-configuration-file","title":"Setting the API URL value in the web configuration file","text":"<p>A web configuration file is used to tell the database which URL should be used for the API endpoint connection.</p> <p>We want the database to use the newly installed SSL certificate / domain.</p> <ol> <li> <p>Download the GetSetCloudAPIURL.bat file from the JWT token error KB article.</p> </li> <li> <p>Run the GetSetCloudAPIURL.bat file and proceed through the Command Prompt tasks.</p> </li> <li> <p>Type in the name of the database exactly as it appears in the list in the Command Prompt window to select it.</p> </li> <li> <p>You are then asked if you want to update the API URL value for this database.</p> <p></p> </li> <li> <p>Type Y for \"yes\" and hit enter. Or type N for \"no\" if you chose the wrong database.</p> </li> <li> <p>Copy and paste (or type in) the full API URL that has been configured with the SSL certificate, including the /act.web.api pathing: <code>https://actweb.mycompany.com/act.web.api</code></p> </li> <li> <p>Hit Enter on your keyboard once the URL has been entered into the Command Prompt properly.</p> </li> <li> <p>The Command Prompt will return a message confirming the change to the configuration value.</p> </li> <li> <p>Exit the Command Prompt.</p> </li> </ol>"},{"location":"Act_ConnectLink_Install/#testing-verifying-the-ssl-certificate","title":"Testing / verifying the SSL certificate","text":"<p>To verify the SSL configuration was completed properly:</p> <ol> <li> <p>Open a web browser and navigate to the domain URL that was configured with the SSL certificate.</p> </li> <li> <p>To test APFW functionality, add /apfw to the end of the URL: <code>https://actweb.companyname.com/apfw</code></p> </li> <li> <p>To test the web API connection, add /act.web.api to the end of the URL: <code>https://actweb.companyname.com/act.web.api</code></p> </li> <li> <p>The URL with /apfw in its path should open a webpage where you can log into the Act! database using Act! user credentials.</p> </li> <li> <p>The URL with /act.web.api in its path will open a webpage displaying the Act! web API help documentation.</p> </li> </ol> <p>If either of these pages do not load as you expect them to, there is likely something in the firewall blocking the connections.</p> <p>To verify that the Act! configuration was completed properly, use the local file directories that have been configured in IIS:</p> <ol> <li> <p>Open a web browser and browse to these two URLs:</p> <ul> <li><code>localhost/apfw</code></li> <li><code>localhost/act.web.api</code></li> </ul> </li> <li> <p>If the Website Administration tasks have been configured successfully, these two options will return the expected webpages: </p> <ul> <li>the login for the Act! database</li> <li>the Act! web API help documentation page</li> </ul> </li> <li> <p>If the local sites load properly, but the domain sites (using the SSL certificate) do not, then something is being blocked regarding the SSL certificate domain or https port (443).</p> </li> <li> <p>If the local sites do not load properly, your Act! Website Administration configuration may be misconfigured somewhere, or the firewall is blocking portions of the Act! application from running properly.</p> </li> </ol>"},{"location":"Act_ConnectLink_Install/#adding-act-web-api-to-a-act-premium-non-web-installation","title":"Adding Act! Web API to a Act! Premium (non-web) installation","text":"<p>If the client's server / host machine is not using Act! Premium for Web (APFW), you will not be able to run an SSL certificate configuration for API connectivity.</p> <p>In order to successfully bind an SSL certificate to an Act! database, you need to install Act! Premium for Web. The APFW installation process creates the necessary virtual directories in IIS that are used with an SSL certificate.</p> <p>Clients can still create an API connection for their non-web Act! database by installing and configuring the Act! Connect Link software on the server / host machine.</p>"},{"location":"AuthBridgeService/","title":"Technical 2","text":""},{"location":"AuthBridgeService/#authentication-bridge-service-client-doc","title":"Authentication Bridge Service - Client Doc","text":""},{"location":"AuthBridgeService/#overview","title":"Overview","text":"<p>This document outlines the processes that both the client and Nelnet must complete in order to successfully authenticate using the Authentication Bridge Service.</p> <p>A successful authentication will provide the client access to the Velocity data APIs.</p>"},{"location":"AuthBridgeService/#what-the-client-provides-to-nelnet","title":"What the client provides to Nelnet","text":"<ul> <li>The public key from a public/private keyset created using the RSA algorithm that is ideally 4096 bits in size (typically the client can request this from the client's IT department)<ul> <li>4096 bits is the target size because larger key sizes will be processed more slowly by the system.</li> <li>The public key created as part of the keyset should be given to Nelnet. Do not give Nelnet the private key.</li> </ul> </li> <li>A JWT minted and signed with the private key from the keyset created that includes the client's Velocity BorrowerId value (GUID)<ul> <li>The client's BorrowerId value (GUID) is provided to them in the OnboardingId (GUID) bundle during the onboarding process.</li> </ul> </li> </ul>"},{"location":"AuthBridgeService/#what-nelnet-provides-to-the-client","title":"What Nelnet provides to the client","text":"<ul> <li>Nelnet creates the authentication bridge records and sends the bridge key to the client.</li> <li>The bridge authentication endpoint URL: <code>https://&lt;ENV&gt;.nelnet.io/authbridgeapi/auth-bridge/authenticate</code><ul> <li>Depending on the environment, replace the <code>&lt;ENV&gt;</code> with the value provided for a specific environment.</li> </ul> </li> </ul>"},{"location":"AuthBridgeService/#process-requirements","title":"Process requirements","text":"<ol> <li> <p>The client must create a new public/private keyset using the RSA algorithm; this should be around 4096 bits in size.</p> </li> <li> <p>The client sends the public key of the keyset to Nelnet.</p> Public Key Example<pre><code>    -----BEGIN PUBLIC KEY-----\n    MIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEA5osWL21cwZ3LWjLTsx5T\n    O9YVb0S3EH3eh4XihJE5SUnalLFzs0ATE1Eaf3NiLeFzOV5uv3U+HudyyT2KCcKo\n    Aw5o2juUfsqDKM5OI1/AhzEwFmnX16LfCiMOJYhCMYNM55m3Cn7z/AmojMq2mbGE\n    CtGAaIGx+2zZlfVLIF4sv8ONvyMxyGmgiwvIjrk44AkwZQN6dZq0XkuZsbtVqZ8p\n    tC/MjPNavY7GuM8YL5OTYdcS6BgSDlGuo1zf7jEH5Qp54uFqrYH75x6llK7QasVe\n    \\nMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEA0YIiAk0FN1Wv\\n9Mww\n    IDAQAB\\n\n    -----END PUBLIC KEY-----\n</code></pre> </li> <li> <p>Nelnet configures the authentication bridge and sends the bridge key to the client.</p> <ul> <li>Example: 418281ac-22dc-4a37-a136-9c377a41da31</li> </ul> </li> <li> <p>The client creates a new JWT that is signed with the private key generated above, using the payload formatting shown in the example here:</p> Minimum JWT Payload<pre><code>    {\n        \"velocityBorrowerId\": \"9d4b59f3-86a3-42f6-8f3e-50272f85d01c\",\n        \"sub\": \"a867824d-58c3-4992-9926-ee9f423e8a93\",\n        \"scp\": \"borrower\",\n        \"iat\": 1567550002,\n        \"exp\": 1583328478\n    }\n</code></pre> </li> <li> <p>The client calls the authentication bridge's authenticate endpoint URL (POST) using the parameters listed in the example shown here:</p> <ul> <li>Authorization: The JWT that was generated from Step 4.</li> <li>Bridge-Key: The bridge key value provided by Nelnet.</li> </ul> cURL<pre><code>    curl -L -X POST 'https://&lt;ENV&gt;.nelnet.io/authbridgeapi/auth-bridge/authenticate' \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer &lt;CLIENT.TOKEN&gt;' \\\n    -H 'Bridge-Key: &lt;CLIENT.BRIDGEKEY&gt;'\n</code></pre> </li> <li> <p>A successful request returns a JSON payload including the data shown in the example here:</p> Response - Status: 201 Created<pre><code>    {\n        \"data\": {\n            \"accessToken\": \"&lt;VELOCITY.BORROWER.TOKEN&gt;\",\n            \"tokenType\": \"Bearer\",\n            \"issuedAt\": 1653672915,\n            \"expiresIn\": 1653676515\n        }\n    }\n</code></pre> </li> </ol>"},{"location":"DataAvailability_Snowflake/","title":"Technical 1","text":""},{"location":"DataAvailability_Snowflake/#data-availability-via-snowflake","title":"Data Availability via Snowflake","text":"<p>This technical document provides a high level understanding of the tooling and systems being used in the team's data analytics efforts and projects.</p> <ul> <li>Velocity is the internal name for the company's loan servicing platform.</li> <li>All sensitive information (database names, specific URLs, etc.) has been anonymized in order to add this content to my portfolio.</li> </ul>"},{"location":"DataAvailability_Snowflake/#overview","title":"Overview","text":"<p>Snowflake is a cloud-based data warehouse that pulls in data from the various Velocity microservice databases. The Data Availability Team (DAT) uses a software called DBT as a tool to create different \"views\" within Snowflake. Each view in Snowflake is a SQL query that has been structured by the DAT to display various data elements in specific ways that are helpful for users when analyzing data. </p> <p>Currently, views within Snowflake display data from the XYZ_DB database. However, there are other databases that exist outside of Velocity that are maintained by other teams (such as Analytics, Ops Analysts, etc.). There are different types of Snowflake views and each type has associated characteristics, including how often the underlying tables are refreshed with new data. </p> <p>The Data Dictionary is a comprehensive database of the various data elements (also known as \"fields\") that live in Velocity and Snowflake with a description of each data element. The Data Dictionary currently exists as an Excel spreadsheet, but efforts are being made to move the Data Dictionary onto the DBT platform for various quality of life reasons, with the main benefit being the ability to access the Data Dictionary field descriptions within Snowflake directly.</p>"},{"location":"DataAvailability_Snowflake/#what-is-snowflake","title":"What is Snowflake?","text":"<p>Snowflake is a cloud-based data warehouse that imports data from a collection of databases that exist in PostgreSQL (Postgres) using AWS Glue and a new tool called Rivery. The Velocity platform is comprised of various microservices that all communicate using various APIs and each microservice contains a collection of tables (or small databases). Velocity has approximately 60 different microservices and corresponding databases in Postgres, which Snowflake can pull from to query different data and then present different data based on the queries.</p> <p>Additionally, Snowflake currently serves as a data warehouse for entities other than just Velocity, meaning it contains a lot of other data that is not maintained specifically by the DAT. If users wish to find data or information about the other areas and tables within Snowflake, they will need to contact the owner of the specific space.</p> <p>Data before Snowflake</p> <pre><code>Velocity launched in 2019 and Snowflake became the preferred data warehouse for Velocity in 2021. \nDue to this timeline, there is approximately 1.5 years worth of data that exists in Velocity that does not exist in Snowflake at this \ntime. There is an old RDS environment (Redshift Postgres) that acted as a temporary solution prior to adopting Snowflake that is still \nactive and houses this older data as well.\n\nThe earliest records in Snowflake have a timestamp of around March of 2021.\n</code></pre>"},{"location":"DataAvailability_Snowflake/#what-is-dbt","title":"What is DBT?","text":"<p>DBT is the software that the Data Availability Team uses as a development environment. Each view that is created by the DAT, and then used by Nelnet users in Snowflake, is developed in DBT. DBT interfaces with Github, allowing the team to use all of the quality of life features that Github has to offer. </p> <p>DBT also acts as a data transformation tool, allowing users to take source data and model it in ways that materialize different views and tables. So, while the DAT uses it primarily for creating and developing different views, its functionality goes beyond simply creating different views.</p> <p>Additionally, DBT serves as the central location for all of the Data Availability Team's column-level documentation. This documentation is currently being added as the DAT touches each column and/or works on new tickets and projects.</p> <p>To learn more about DBT, you can visit the DBT website.</p>"},{"location":"DataAvailability_Snowflake/#what-is-the-data-dictionary","title":"What is the Data Dictionary?","text":"<p>The Data Dictionary is a project with the primary goal of providing helpful descriptions for all of the various data elements that exist within Velocity today. For example, if a user were to query one of the Velocity APIs and was returned a JSON payload full of data, the Data Dictionary could be used to look up the different data element names and their descriptions. </p> <p>While the Data Dictionary currently exists in Excel spreadsheets, there are efforts underway to move the existing data element descriptions into a DBT repository. Doing this would centralize the Data Dictionary for all users, allow data element descriptions to be updated in a live database, allow Snowflake users to access the Data Dictionary within Snowflake, and allow DBT users to see the relationships between data elements and the different views they are used in (otherwise known as their lineage).</p>"},{"location":"DataAvailability_Snowflake/#read-only-vs-full-access-users","title":"Read-only vs. full access users","text":"<p>For security purposes (as well as financial purposes), there are two instances of Snowflake that a user can log in to: </p> <ul> <li>Read-only</li> <li>Full access</li> </ul> <p>The role and permissions given to each user will determine which instance of Snowflake they will primarily be using.</p> <p>The URLs for each instance are listed here:</p> <ul> <li>Read-only: https://xxxxxxxx.snowflakecomputing.com/</li> <li>Full access: https://app.snowflake.com/us-west/xxxxxxxxx/</li> </ul> <p>The main difference between the two databases is that the full access database makes more tables visible to the user.</p> <p>Full-access users are more expensive to add to a license, meaning most users will be given read-only access.</p>"},{"location":"DataAvailability_Snowflake/#snowflake-views-and-table-refresh-times","title":"Snowflake views and table refresh times","text":"<p>As Snowflake is a data warehouse containing the collection of Velocity microservice databases, it provides a powerful tool in being able to query data across the entire Velocity platform and present the data in easy-to-read views.</p> <p>One caveat with Snowflake views is that not all data in Velocity is available in Snowflake. This is often because old data elements never got added to views or because newly developed data elements have yet to be added to views.</p> <p>Currently, views within Snowflake display data from the XYZ database. However, there are other databases that exist outside of Velocity that might have information that is unrelated to Velocity; or is Velocity-related, but maintained by other teams (such as Analytics, Ops Analysts, etc.).</p> <p>Info</p> <pre><code>The views in Snowflake are non-materialized, meaning each view will re-run the SQL query that generates it and display the \ncurrent data when called.\n\nCreating Snowflake views is necessary because people need to see the various pieces of data without having access to the \nactual tables where the data is housed.\n</code></pre> <p>The different types of views currently available in Snowflake for the XYZ database are:</p> View Type / Name Scheduled table refresh times Description (VW) View Financial data updates as new data comes in from Velocity.Non-financial data updates during the scheduled table refresh times.Starts: 5:00a UTC / 11:00p CSTEnds: 11:20a UTC / 5:20a CST Views that have \"VW\" in their name represent queries that are pulling data in near real-time. The data in these views are updated as the data in Velocity changes.Non-financial related data changes (that is, address changes, phone number changes, e-correspondence/SCRA status changes, etc.) do not update automatically when changes to those pieces of data are made in Velocity. Changes to these pieces of information will only update in the Snowflake views during the refresh cycle each morning. (DS) Daily snapshot Starts: 5:00a UTC / 11:00p CSTEnds: 11:20a UTC / 5:20a CST Views that have \"DS\" in their name represent queries that are snapshotting data elements at a scheduled time, meaning the information is going to be accurate as of the time it was last queried. These views are built from AWS Glue (and Rivery) tables brought over nightly. (RP) Reports Starts: 5:00a UTC / 11:00p CSTEnds: 11:20a UTC / 5:20a CST Views that have \"RP\" in their name are reporting views based off of \"VW\" and \"DS\" views. (CRP) Citizens reports Starts: 5:00a UTC / 11:00p CSTEnds: 11:20a UTC / 5:20a CST Views that have \"CRP\" in their name are the same as \"RP\" views, but are specific to Client. <p>The DAT is currently working on moving all table refresh times to a single refresh that will occur at 7:00a UTC using Rivery, but until they are finished, please use the resources here for reference.</p> <p>Within the XYZ database there are two types of tables:</p> <ul> <li>One that refreshes in near real time as new data comes into Velocity (this is a singular table, but is very broad)</li> <li>All other tables that are refreshed on a schedule (starting at 11:00p CST and refreshing every so often until 5:20a CST)</li> </ul>"},{"location":"DataAvailability_Snowflake/#touch-events","title":"Touch events","text":"<p>One caveat for the table that refreshes as new data comes into Velocity (near real time) is that non-financial related data changes (that is, address changes, phone number changes, boolean status changes, etc.) do not update automatically when changes to those pieces of data are made in Velocity. Changes to these pieces of information will only update in the Snowflake views during the refresh cycle each morning.</p> <p>Because of this, special requests can be made to refresh the VW Snowflake views using Touch Events, which are pushed by the Data Availability Team to manually update views with the latest changes to data in the tables.</p> <p>Scenarios that most commonly require touch events include:</p> <ul> <li>Investor transfers \u2013 A loan or group of loans have been transferred to a new investor and the views need to be manually refreshed.</li> <li>Loans with a $0 balance, but are still in repayment status \u2013 This is a known issue and is due to the current API occasionally failing to update these loan records appropriately.<ul> <li>Note: This touch event will be automated using Rivery at some point and will not be manually conducted once the automation is in place.</li> </ul> </li> </ul>"},{"location":"DataAvailability_Snowflake/#the-databases","title":"The databases","text":"<p>The two primary databases used in Snowflake are ABC_DB and ANALYTICS_DB. These databases contain all of the raw data.</p> <p>XYZ_DB is a separate database that contains the views that sit on top of the raw data tables. </p>"},{"location":"DataAvailability_Snowflake/#the-abc_db-database-and-xyz_db-database","title":"The ABC_DB database and XYZ_DB database","text":"<p>Raw Velocity data coming into Snowflake is stored in the ABC_DB database, but Snowflake views are stored in the XYZ_DB database.</p> <p>XYZ_DB has two different schemas:</p> <ul> <li>\"LENDER\" for Velocity Servicing</li> <li>\"ORIGINATIONS\" for Velocity Originations</li> </ul> <p>One difference between the two schemas is that the Snowflake views for LENDER pull data from the Postgres database, while ORIGINATIONS views use a source file that gets created by the Originations system. This source file gets loaded into an AWS S3 bucket before being picked up by Snowflake (using the Snowpipe service) where changes in data are then loaded into the ORIGINATION_SOURCE table. It is this table that is used to create the views in Snowflake for Velocity Originations.</p>"},{"location":"DataAvailability_Snowflake/#the-analytics_db-database","title":"The ANALYTICS_DB database","text":"<p>The ANALYTICS_DB database is used by the Analytics team, as well as the Business, for reporting purposes. A list of these reports can be found in Snowflake in the Reports folder of the ANALYTICS_DB database.</p>"},{"location":"DataAvailability_Snowflake/#external-data-sources","title":"External data sources","text":"<p>Snowflake Marketplace allows our users a connection to hundreds of data providers, offering thousands of ready-to-use data resources.</p> <p>Velocity and Analytics use a calendar data source to aid reporting efforts using date driven reports. Specifically, they use \"Calendars for Financial and Analytics\" provided by Mondo Analytics.</p> <p>They are accessible via four views, and are installed on the four main Snowflake accounts:</p> <ul> <li>XYXYXCHCH (dev and train)</li> <li>YZYZWEWEW (prod)</li> <li>HGHGHGXXX (analytics reader train)</li> <li>FV12DCH (analytics reader prod)</li> </ul> <p>The calendar location is in the FINANCIAL_CALENDARS database, in the PUBLIC schema, for all accounts.</p>"},{"location":"EZLinks_Capital_Proposal/","title":"Proposal","text":""},{"location":"EZLinks_Capital_Proposal/#capital-expense-project-proposal","title":"Capital Expense Project Proposal","text":"<p>This short document was created as part of a proposal from the Information Systems department to move away from one software and replace it with another. It covers the overall cost estimates, pros and cons for the project, current pain points, and a narrative explaining the objectives and reasoning for the proposed solution.</p>"},{"location":"EZLinks_Capital_Proposal/#retail-ezlinks-capital-proposal","title":"Retail EZLinks Capital Proposal","text":""},{"location":"EZLinks_Capital_Proposal/#hardware-costs","title":"Hardware Costs","text":"Item Quantity Cost (One-time) Cost (Total) MC2100 Model Scan Guns (or similar) 11 ~ $9,075 ~ $9,075 3-Year Service Warranty 11 ~ $2,860 ~ $2,860 MC2100 Scan Gun Cradles + Accessories 6 ~ $1,694 ~ $1,694 Total - ~ $13,629 ~ $13,629 <ul> <li>Cost per scan gun: ~ $1,085 for IBS / ~ $1,720 for RTP</li> </ul>"},{"location":"EZLinks_Capital_Proposal/#software-costs","title":"Software Costs","text":"Item Quantity Cost (Monthly) Cost (Annually) IBS Software License / Maintenance &amp; Support 6 $60 each $4,320 IBS Retail Inventory Modules - - $0 Total - - $4,320 Item Quantity Cost (Monthly) Cost (Annually) RTP Software License / Maintenance &amp; Support 6 $65 each $4,680 Active Network Retail Inventory Modules - - $2,415 Total - - $7,095 <ul> <li>IBS software license and maintenance &amp; support cost $60/month each, RTP costs $65/month each.</li> <li>Active Network Retail Inventory Modules are additional for RTP, costing $2,415 annually.</li> </ul>"},{"location":"EZLinks_Capital_Proposal/#current-rtp-retail-deficiencies","title":"Current RTP Retail deficiencies","text":"<ul> <li>Scan guns are reaching life expectancy</li> <li>RTP scan guns experience more problems than those used with IBS / EZLinks</li> <li>Active Network support not as strong as EZLinks support</li> <li>Unable to view retail variance in real-time</li> <li>Poor membership experience; EZLinks is our preferred membership software</li> <li>Must manually input member billing account charges from RTP to EZLinks</li> </ul>"},{"location":"EZLinks_Capital_Proposal/#benefits-of-moving-to-ezlinks-for-retail","title":"Benefits of moving to EZLinks for Retail","text":"<ul> <li>Less problematic scan guns</li> <li>EZLinks is used by our summer retail operations and our membership team</li> <li>Members will be able to view purchases / receipts online</li> <li>More user friendly for staff</li> </ul>"},{"location":"EZLinks_Capital_Proposal/#cons-of-moving-to-ezlinks-for-retail","title":"Cons of moving to EZLinks for Retail","text":"<ul> <li>We lose resort charge functionality at retail locations</li> </ul>"},{"location":"EZLinks_Capital_Proposal/#proposed-solution","title":"Proposed solution","text":"<p>Retail operations at Telluride Ski and Golf are currently split between two different software: RTP and IBS. While our Golf Pro Shop uses IBS in the summer months, the bulk of our retail operations, including The Resort Store, Heritage and Telluride Naturals, all operate using RTP. RTP excels in certain environments, such as the Pass Office, Skier Visit Reporting, and F&amp;B, but seems to fall short when it comes to retail and inventory, comparatively.</p> <p>The current model of scan gun that we must use (due to compatibility reasons) for RTP retail operations are approaching five years of use, and we are experiencing more and more problems with them as the days pass. Because the support warranties for the guns are about to expire, we think it is time to decide on a new direction for retail.</p> <p>There are pros and cons to switching over to IBS, but in our opinion, the pros far outweigh the cons. </p> <p>To start, the MC2100 model of scan gun that the Golf Pro Shop uses in conjunction with IBS has seen fewer problems over the years when compared to RTP and the MC3200 model of scan gun, and IBS / EZLinks support has demonstrated a much better effort to help support our retail operations.</p> <p>Additionally, we will be able to see retail variance in real time (instead of having to wait for inventory batches to upload overnight), and IBS is our main software for managing club memberships, which means members will be able to view receipts for any purchases made when paying their membership bill online. Other benefits include not needing to manually input billing account charges from RTP into IBS (which saves money on labor), as well as some of our RTP software licenses getting freed up, allowing us to use them elsewhere (such as Gorrono) when/if we need them.</p> <p>The cons of switching to IBS include losing features such as Resort Charge and any RTP-integrated gift cards, as well as having to train staff on IBS / EZLinks software. Any RTP gift cards that are in circulation now will not function at IBS / EZlinks retail locations. We could implement a work-around, but feel that it would cause more problems than if we were to not do it.</p> <p>We believe that it is time to move away from RTP as our main retail software, and subsequently, move away from these specific scan guns as well. Utilizing IBS / EZLinks software for our retail operations will save time and money, provide a better experience for our retail staff and will ultimately provide a better overall experience for our members.</p>"},{"location":"MTS_Newsletter_Edits/","title":"Editing","text":"<p>This newsletter was written by a colleague for a conference that our organization sponsored. </p> <p>Comments with feedback and suggested edits were provided by myself.</p> <p></p> <p></p>"}]}